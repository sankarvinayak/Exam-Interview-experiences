Explain your academic background
-
What is GD
-
Gradient descent is a technique used to optization. The error surface may contain many peak and wally, it take gradient at a point and move in the direction which decrease error

Does GD anways converge go global minima
-
No,It may also end up in local minima

How to choose learning rate
-
we can try different values. Start with larger

But it increase the computation right
-
Yes. Then we can try techniques like AdaGrad..

Don't go to that complex things
-
We can start with a larger value and once the error stop to go down decrease it.




What is SGD
-
Advantage over GD
-
Properties of PDF
-
properties of CDF
-

Time complexity of multiplying matrix m1 with s1 nonzero elements and m2 with s2 non zero elements
-

Properties of AA<sup>T</sup>
-
